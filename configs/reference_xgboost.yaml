
experiment: my_run
patchsize: 0
memory_fraction: 0.5

features:
  - name: my continuous features
    type: continuous
    files:
      - path: configs/data/sirsam/er_depg.tif
      - path: configs/data/sirsam/sagawet_b_sir.tif
      - path: configs/data/sirsam/dem_foc2.tif
      - path: configs/data/sirsam/outcrop_dis2.tif
#      - path: configs/data/sirsam/k_15v5.tif
      - path: configs/data/sirsam/relief_apsect.tif
#      - path: configs/data/sirsam/LATITUDE_GRID1.tif
#      - path: configs/data/sirsam/LONGITUDE_GRID1.tif
#      - directory: configs/data/weights/
    transforms:
      - identity
    #      - whiten:
    #          keep_fraction: 0.98
    imputation: none

preprocessing:
  imputation: none
  transforms:
#    - whiten:
#        keep_fraction: 0.98

targets:
  file: configs/data/geochem_sites.shp
  property: K_ppm_imp
#  weight_col_name: K_ppm_imp_
#  file: configs/data/weights/Gamma_K_50.shp
#  property: K2O
#  weight_col_name: weight
#  group_targets:
#    group_col: weight
#    groups_eps: 0.09


# optimisation note:

#scoring can be any of:
#  explained_variance,
#  max_error,
#  neg_mean_absolute_error,
#  neg_mean_squared_error,
#  neg_root_mean_squared_error
#  neg_mean_squared_log_error
#  neg_median_absolute_error
#  r2
#  neg_mean_poisson_deviance
#  neg_mean_gamma_deviance
#  neg_mean_absolute_percentage_error


learning:
  algorithm: xgboost
  arguments:
    target_transform: identity
    booster: dart
    colsample_bylevel: 0.576015640010833
    colsample_bynode: 0.7753052852726197
    colsample_bytree: 0.7551261614451765
    gamma: 1.608938052610619
    learning_rate: 0.7157991177877647
    max_delta_step: 4
    max_depth: 10
    min_child_weight: 0
    n_estimators: 24
    reg_alpha: 14.166999106742683
    reg_lambda: 261.0013846007669
    subsample: 0.51376699456898
    random_state: 3
  optimisation:
#    searchcv_params:
#      n_iter: 5
#      cv: 2
#      verbose: 1000
#      n_points: 3
#      n_jobs: 12
#      random_state: 4
#      scoring: r2
#    params_space:
#      max_depth: Integer(1, 15)
#      learning_rate: Real(10 ** -5, 10 ** 0, prior="log-uniform")
#      n_estimators: Integer(50, 200)
#      min_child_weight: Integer(1, 10)
#      max_delta_step: Integer(0, 10)
#      gamma: Real(0, 0.5, prior="uniform")
#      colsample_bytree: Real(0.3, 0.9, prior="uniform")
#      subsample: Real(0.01, 1.0, prior='uniform')
#      colsample_bylevel: Real(0.01, 1.0, prior='uniform')
#      colsample_bynode: Real(0.01, 1.0, prior='uniform')
#      reg_alpha: Real(1, 100, prior='uniform')
#      reg_lambda: Real(0.01, 10, prior='log-uniform')
    hyperopt_params:
      max_evals: 5
      step: 2
      cv: 2
      verbose: true
      random_state: 3
      scoring: r2  # r2, neg_mean_absolute_error, etc..see note above
      algo: bayes   # bayes, or anneal
    hp_params_space:
      max_depth: randint('max_depth', 1, 15)
      n_estimators: randint('n_estimators', 5, 25)
      learning_rate: loguniform('learning_rate', -5, 0)
      booster:  choice('booster', ['gbtree', 'dart'])
      min_child_weight: randint('min_child_weight', 0, 10)
      max_delta_step: randint('max_delta_step', 0, 20)
      gamma: uniform('gamma', 0, 10)
      subsample: uniform('subsample', 0.01, 1.0)
      colsample_bytree: uniform('colsample_bytree', 0.01, 1.0)
      colsample_bylevel: uniform('colsample_bylevel', 0.01, 1.0)
      colsample_bynode: uniform('colsample_bynode', 0.01, 1.0)
      reg_alpha: uniform('reg_alpha', 0, 100)
      reg_lambda: loguniform('reg_lambda', 0.01, 10)


prediction:
  quantiles: 0.95
  outbands: 1

validation:
  #- feature_rank
  - parallel
#  - permutation_importance
#  - feature_importance
  - k-fold:
      folds: 3
      random_seed: 1

oos_validation:
  file: configs/data/weights/Gamma_K_50.shp
  property: K2O

output:
  directory: ref_xgb/

